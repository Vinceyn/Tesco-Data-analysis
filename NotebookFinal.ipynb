{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 P-ADA-wan ADA Project: Milestone 4\n",
    "\n",
    "\n",
    "This notebook is part of the Fall 2020 EPFL ADA course. On this notebook, we'll try to extend the Tesco's paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Machine Learning \n",
    "\n",
    "\n",
    "We will first try to create correct Machine Learning algorithms which will link Tesco's grocery data with children obesity, then extract meaningful informations from our results.\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "As usual, we will import our libraries and our dataset in our notebook. The data was taken from [The official paper's dataset](https://figshare.com/collections/Tesco_Grocery_1_0/4769354/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model, model_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files into DataFrames\n",
    "obesity = pd.read_csv(Path.cwd() / \"data\" / \"child_obesity_london_ward_2013-2014.csv\")\n",
    "grocery = pd.read_csv(Path.cwd() / \"data\" / \"year_osward_grocery.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>weight_perc2.5</th>\n",
       "      <th>weight_perc25</th>\n",
       "      <th>weight_perc50</th>\n",
       "      <th>weight_perc75</th>\n",
       "      <th>weight_perc97.5</th>\n",
       "      <th>weight_std</th>\n",
       "      <th>weight_ci95</th>\n",
       "      <th>volume</th>\n",
       "      <th>...</th>\n",
       "      <th>man_day</th>\n",
       "      <th>population</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>age_0_17</th>\n",
       "      <th>age_18_64</th>\n",
       "      <th>age_65+</th>\n",
       "      <th>avg_age</th>\n",
       "      <th>area_sq_km</th>\n",
       "      <th>people_per_sq_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E05000026</td>\n",
       "      <td>449.535137</td>\n",
       "      <td>32.5</td>\n",
       "      <td>166.4</td>\n",
       "      <td>300.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>771.349423</td>\n",
       "      <td>3.716832</td>\n",
       "      <td>125.960833</td>\n",
       "      <td>...</td>\n",
       "      <td>37315</td>\n",
       "      <td>14370.0</td>\n",
       "      <td>7469.0</td>\n",
       "      <td>6901.0</td>\n",
       "      <td>4211.0</td>\n",
       "      <td>9421.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>29.572999</td>\n",
       "      <td>1.26</td>\n",
       "      <td>11404.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E05000027</td>\n",
       "      <td>413.130263</td>\n",
       "      <td>32.5</td>\n",
       "      <td>150.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>626.395178</td>\n",
       "      <td>5.164174</td>\n",
       "      <td>110.664114</td>\n",
       "      <td>...</td>\n",
       "      <td>14474</td>\n",
       "      <td>10845.0</td>\n",
       "      <td>5228.0</td>\n",
       "      <td>5617.0</td>\n",
       "      <td>3205.0</td>\n",
       "      <td>6608.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>33.568004</td>\n",
       "      <td>1.36</td>\n",
       "      <td>7974.264706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E05000028</td>\n",
       "      <td>407.100472</td>\n",
       "      <td>32.5</td>\n",
       "      <td>160.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>545.890959</td>\n",
       "      <td>2.710677</td>\n",
       "      <td>121.990710</td>\n",
       "      <td>...</td>\n",
       "      <td>32138</td>\n",
       "      <td>13856.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>7106.0</td>\n",
       "      <td>4180.0</td>\n",
       "      <td>8537.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>32.032693</td>\n",
       "      <td>1.29</td>\n",
       "      <td>10741.085271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05000029</td>\n",
       "      <td>384.173858</td>\n",
       "      <td>30.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>591.837557</td>\n",
       "      <td>4.968373</td>\n",
       "      <td>122.245578</td>\n",
       "      <td>...</td>\n",
       "      <td>16223</td>\n",
       "      <td>10850.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>3023.0</td>\n",
       "      <td>6251.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>36.004793</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3210.059172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E05000030</td>\n",
       "      <td>356.882607</td>\n",
       "      <td>30.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>465.284180</td>\n",
       "      <td>3.880963</td>\n",
       "      <td>109.959688</td>\n",
       "      <td>...</td>\n",
       "      <td>17522</td>\n",
       "      <td>11348.0</td>\n",
       "      <td>5515.0</td>\n",
       "      <td>5833.0</td>\n",
       "      <td>2747.0</td>\n",
       "      <td>6961.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>37.247444</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3289.275362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     area_id      weight  weight_perc2.5  weight_perc25  weight_perc50  \\\n",
       "0  E05000026  449.535137            32.5          166.4          300.0   \n",
       "1  E05000027  413.130263            32.5          150.0          300.0   \n",
       "2  E05000028  407.100472            32.5          160.0          300.0   \n",
       "3  E05000029  384.173858            30.0          150.0          250.0   \n",
       "4  E05000030  356.882607            30.0          140.0          250.0   \n",
       "\n",
       "   weight_perc75  weight_perc97.5  weight_std  weight_ci95      volume  ...  \\\n",
       "0          500.0           1500.0  771.349423     3.716832  125.960833  ...   \n",
       "1          500.0           1500.0  626.395178     5.164174  110.664114  ...   \n",
       "2          500.0           1200.0  545.890959     2.710677  121.990710  ...   \n",
       "3          454.0           1500.0  591.837557     4.968373  122.245578  ...   \n",
       "4          450.0           1000.0  465.284180     3.880963  109.959688  ...   \n",
       "\n",
       "   man_day  population    male  female  age_0_17  age_18_64  age_65+  \\\n",
       "0    37315     14370.0  7469.0  6901.0    4211.0     9421.0    738.0   \n",
       "1    14474     10845.0  5228.0  5617.0    3205.0     6608.0   1032.0   \n",
       "2    32138     13856.0  6750.0  7106.0    4180.0     8537.0   1139.0   \n",
       "3    16223     10850.0  5300.0  5550.0    3023.0     6251.0   1576.0   \n",
       "4    17522     11348.0  5515.0  5833.0    2747.0     6961.0   1640.0   \n",
       "\n",
       "     avg_age  area_sq_km  people_per_sq_km  \n",
       "0  29.572999        1.26      11404.761905  \n",
       "1  33.568004        1.36       7974.264706  \n",
       "2  32.032693        1.29      10741.085271  \n",
       "3  36.004793        3.38       3210.059172  \n",
       "4  37.247444        3.45       3289.275362  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a closer look at the dataframes\n",
    "grocery.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_id</th>\n",
       "      <th>number_reception_measured</th>\n",
       "      <th>number_y6_measured</th>\n",
       "      <th>prevalence_overweight_reception</th>\n",
       "      <th>prevalence_overweight_y6</th>\n",
       "      <th>prevalence_obese_reception</th>\n",
       "      <th>prevalence_obese_y6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E05000026</td>\n",
       "      <td>712</td>\n",
       "      <td>441</td>\n",
       "      <td>0.259831461</td>\n",
       "      <td>0.387755102</td>\n",
       "      <td>0.133426966</td>\n",
       "      <td>0.247165533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E05000027</td>\n",
       "      <td>592</td>\n",
       "      <td>427</td>\n",
       "      <td>0.224662162</td>\n",
       "      <td>0.402810304</td>\n",
       "      <td>0.099662162</td>\n",
       "      <td>0.259953162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E05000028</td>\n",
       "      <td>665</td>\n",
       "      <td>499</td>\n",
       "      <td>0.24962406</td>\n",
       "      <td>0.466933868</td>\n",
       "      <td>0.123308271</td>\n",
       "      <td>0.29258517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05000029</td>\n",
       "      <td>484</td>\n",
       "      <td>362</td>\n",
       "      <td>0.241735537</td>\n",
       "      <td>0.422651934</td>\n",
       "      <td>0.128099174</td>\n",
       "      <td>0.245856354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E05000030</td>\n",
       "      <td>446</td>\n",
       "      <td>320</td>\n",
       "      <td>0.257847534</td>\n",
       "      <td>0.378125</td>\n",
       "      <td>0.125560538</td>\n",
       "      <td>0.215625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     area_id number_reception_measured number_y6_measured  \\\n",
       "0  E05000026                       712                441   \n",
       "1  E05000027                       592                427   \n",
       "2  E05000028                       665                499   \n",
       "3  E05000029                       484                362   \n",
       "4  E05000030                       446                320   \n",
       "\n",
       "  prevalence_overweight_reception prevalence_overweight_y6  \\\n",
       "0                     0.259831461              0.387755102   \n",
       "1                     0.224662162              0.402810304   \n",
       "2                      0.24962406              0.466933868   \n",
       "3                     0.241735537              0.422651934   \n",
       "4                     0.257847534                 0.378125   \n",
       "\n",
       "  prevalence_obese_reception prevalence_obese_y6  \n",
       "0                0.133426966         0.247165533  \n",
       "1                0.099662162         0.259953162  \n",
       "2                0.123308271          0.29258517  \n",
       "3                0.128099174         0.245856354  \n",
       "4                0.125560538            0.215625  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obesity.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = grocery.merge(obesity, how='inner', on='area_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 202 potentials predictors and 7 potential labels, for 544 datapoints\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(grocery.columns)} potentials predictors and {len(obesity.columns)} potential labels, for {len(joined_df)} datapoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the correct columns\n",
    "\n",
    "As we see with the output of the previous cell, we have a problem. Indeed, we have too many columns for our number of datapoints. With 202 predictors for 544 datapoints, our model will not represent properly the reality.\n",
    "\n",
    "Hence, we should select the columns that will be meaningful to predict our output. But actually, what's our output ? \n",
    "\n",
    "When we read the [Tesco's paper reference](https://figshare.com/articles/dataset/Validation_data_obesity_diabetes_/7796672?backTo=/collections/Tesco_Grocery_1_0/4769354) we see that we have 4 interesting columns, which represent the prevalence of overweight and obese for children of year 6 and children of year 4 and 5.\n",
    "\n",
    "As the data for the overweight children will be more sparse than the data for the obese children (obese children will be included in the overweight ones -> we will always have a greater value for overweight children than for obese), we'll choose to predict the overweight children. We'll also choose to select the data for year 6 children, as we tend to give more liberty to food consumption's of children when they age.\n",
    "\n",
    "So our labeled data will be the column \"prevalence_overweight_y6\".\n",
    "\n",
    "As we take data from londonese children, we'll follow [the guidelines of the National Health Service](https://www.nhs.uk/conditions/obesity/causes/) to select our predictors. We can note that there are four points that we can extract from the dataset that causes obesity: calories, fat, sugar and alcohol. As children of 6 years old are not likely to drink alcohol, we will focus on the 3 columns _fat_, _sugar_ and _energy_tot_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try : follow NHS guidelines on nutrients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fat</th>\n",
       "      <th>sugar</th>\n",
       "      <th>energy_tot</th>\n",
       "      <th>prevalence_overweight_y6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.488797</td>\n",
       "      <td>10.966213</td>\n",
       "      <td>187.114757</td>\n",
       "      <td>0.387755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.733634</td>\n",
       "      <td>10.514427</td>\n",
       "      <td>188.638145</td>\n",
       "      <td>0.402810304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.216310</td>\n",
       "      <td>10.690272</td>\n",
       "      <td>186.861792</td>\n",
       "      <td>0.466933868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.700137</td>\n",
       "      <td>12.938606</td>\n",
       "      <td>190.783934</td>\n",
       "      <td>0.422651934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.381808</td>\n",
       "      <td>11.332898</td>\n",
       "      <td>188.136201</td>\n",
       "      <td>0.378125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fat      sugar  energy_tot prevalence_overweight_y6\n",
       "0  9.488797  10.966213  187.114757              0.387755102\n",
       "1  9.733634  10.514427  188.638145              0.402810304\n",
       "2  9.216310  10.690272  186.861792              0.466933868\n",
       "3  9.700137  12.938606  190.783934              0.422651934\n",
       "4  9.381808  11.332898  188.136201                 0.378125"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = joined_df[[\"fat\", \"sugar\", \"energy_tot\", \"prevalence_overweight_y6\"]]\n",
    "data_1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained the DataFrame containing the informations we wanted to.\n",
    "\n",
    "We now have to preprocess our input. To this end, we will remove the NA values from the target, standardize our predictor, and split the original data into a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitazing NA data. As they are considered as string in the original dataset, we have to remove them as string, then convert it to float.\n",
    "data_1 = data_1[data_1[\"prevalence_overweight_y6\"] != \"na\"]\n",
    "\n",
    "X_1 = data_1[[\"fat\", \"sugar\", \"energy_tot\"]]\n",
    "Y_1 = data_1[\"prevalence_overweight_y6\"].astype(float)\n",
    "\n",
    "X_1 = (X_1 - np.mean(X_1, axis=0)) / np.std(X_1, axis=0)\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, Y_1, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Machine Learning Models\n",
    "\n",
    "Now that we have our data prepared, we will now compute different ML algorithms in order to find out the one that perform the best.\n",
    "\n",
    "In our case, we will use the [R-squared](https://www.geeksforgeeks.org/ml-r-squared-in-regression-analysis/) to estimate the efficiency of our ML algorithm. To give a quick recap, the R-squared represent the value obtained when comparing the residual sum of squares (SSres) with total sum of squares(SStot). If the model fit the data perfectly, the value obtained will be 1. If we create an average line and present it as our model, the value obtained will be 0. Hence, everything between 0 and 1 means that our model is better than drawing a straight line, and everything below 0 means that our model is worse than an average line (which is not what we want to have).\n",
    "\n",
    "At the same time, we will get the MSE error, to have a second comparison points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def linear_regression(X_train, y_train, X_test, y_test):\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model = linear_model.fit(X_train, y_train)\n",
    "    linear_model_prediction = linear_model.predict(X_test)\n",
    "\n",
    "    print(f\"Mean squared error: {mean_squared_error(y_test, linear_model_prediction):.3f}\")\n",
    "    print(f\"R2 score: {r2_score(y_test, linear_model_prediction):.3f}\")\n",
    "    print(f\"coefs {linear_model.coef_}\")\n",
    "\n",
    "\n",
    "# Ridge model\n",
    "def ridge_model(X_train, y_train, X_test, y_test):\n",
    "    alphas = np.arange(0.1, 10, 0.1)\n",
    "    best_r2_score_ridge = -99\n",
    "    mse_score_related_ridge = 0\n",
    "    best_alpha = 0\n",
    "\n",
    "\n",
    "    for alpha in alphas:\n",
    "        ridge_model = linear_model.Ridge(alpha=alpha)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        ridge_model_prediction = ridge_model.predict(X_test)\n",
    "        r2 = r2_score(y_test, ridge_model_prediction)\n",
    "        mse = mean_squared_error(y_test, ridge_model_prediction)\n",
    "        if r2 > best_r2_score_ridge:\n",
    "            best_r2_score_ridge = r2\n",
    "            mse_score_related_ridge = mse\n",
    "            best_alpha = alpha\n",
    "\n",
    "    print(f\"A ridge model obtain the best r2 score when the lasso alpha is {best_alpha}. The r2 score is {best_r2_score_ridge:.3f} and the related mse score is {mse_score_related_ridge:.3f}\")\n",
    "    \n",
    "def svm_model(X_train, y_train, X_test, y_test):\n",
    "    svm_model = svm.SVR()\n",
    "    svm_model = svm_model.fit(X_train, y_train)\n",
    "    svm_model_prediction = svm_model.predict(X_test)\n",
    "\n",
    "    print(f\"SVM: Mean squared error: {mean_squared_error(y_test, svm_model_prediction):.3f}\")\n",
    "    print(f\"SVM: R2 score: {r2_score(y_test, svm_model_prediction):.3f}\")\n",
    "\n",
    "    \n",
    "def tree_model(X_train, y_train, X_test, y_test):\n",
    "    tree_model = tree.DecisionTreeRegressor()\n",
    "    tree_model = tree_model.fit(X_train, y_train)\n",
    "    tree_model_prediction = tree_model.predict(X_test)\n",
    "\n",
    "    print(f\"Decision Tree: Mean squared error: {mean_squared_error(y_test, tree_model_prediction):.3f}\")\n",
    "    print(f\"Decision Tree: R2 score: {r2_score(y_test, tree_model_prediction):.3f}\")\n",
    "\n",
    "def GBR_model(X_train, y_train, X_test, y_test):\n",
    "    GBR_model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    GBR_model = GBR_model.fit(X_train, y_train)\n",
    "    GBR_model_prediction = GBR_model.predict(X_test)\n",
    "\n",
    "    print(f\"Gradient Boosting Regressor: Mean squared error: {mean_squared_error(y_test, GBR_model_prediction):.3f}\")\n",
    "    print(f\"Gradient Boosting Regressor::R2 score: {r2_score(y_test, GBR_model_prediction):.3f}\")\n",
    "    \n",
    "def ADA_model(X_train, y_train, X_test, y_test):\n",
    "    ADA_model = AdaBoostRegressor(n_estimators=10)\n",
    "    ADA_model = ADA_model.fit(X_train, y_train)\n",
    "    ADA_model_prediction = ADA_model.predict(X_test)\n",
    "\n",
    "    print(f\"ADA: Mean squared error: {mean_squared_error(y_test, ADA_model_prediction):.3f}\")\n",
    "    print(f\"ADA: R2 score: {r2_score(y_test, ADA_model_prediction):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now try our ML algorithm on our data ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.004\n",
      "R2 score: -0.064\n",
      "coefs [-0.01662939 -0.01028952  0.02897609]\n",
      "A ridge model obtain the best r2 score when the lasso alpha is 9.9. The r2 score is -0.049 and the related mse score is 0.004\n",
      "SVM: Mean squared error: 0.004\n",
      "SVM: R2 score: -0.256\n",
      "Decision Tree: Mean squared error: 0.008\n",
      "Decision Tree: R2 score: -1.357\n",
      "Gradient Boosting Regressor: Mean squared error: 0.004\n",
      "Gradient Boosting Regressor::R2 score: -0.233\n",
      "ADA: Mean squared error: 0.004\n",
      "ADA: R2 score: -0.134\n"
     ]
    }
   ],
   "source": [
    "linear_regression(X_train_1, y_train_1, X_test_1, y_test_1)\n",
    "ridge_model(X_train_1, y_train_1, X_test_1, y_test_1)\n",
    "svm_model(X_train_1, y_train_1, X_test_1, y_test_1)\n",
    "tree_model(X_train_1, y_train_1, X_test_1, y_test_1)\n",
    "GBR_model(X_train_1, y_train_1, X_test_1, y_test_1)\n",
    "ADA_model(X_train_1, y_train_1, X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the results\n",
    "It seems that the model performs poorly. Indeed, the r2 score is almost always negative, except for the ADA algorithm (can we state that ADA is the most efficient way to do machine learning then ? :D), when the r2 score is almost null at 0.04.\n",
    "\n",
    "So our initial predictors do not perform well with our target. Before giving it up, let's try to still follow the NHS guidelines which points calories, sugar and fat for obesity causes.\n",
    "\n",
    "## Second try : NHS guidelines, but with relative values instead of absolutes\n",
    "\n",
    "This time, we'll try to perform our model with _f\\_energy\\_sugar_ and _f\\_energy\\_fat_ instead of _sugar_ and _fat_. These predictors represent the proportion of energy of fat and sugar in the average product bought by Tesco's customer, as opposed to the previous predictors which represented the absolute value of those nutrients in the average food product.\n",
    "\n",
    "### Data processing\n",
    "\n",
    "We will select the corresponding columns from the DataFrame, and preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_energy_fat</th>\n",
       "      <th>f_energy_sugar</th>\n",
       "      <th>energy_tot</th>\n",
       "      <th>prevalence_overweight_y6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.234428</td>\n",
       "      <td>187.114757</td>\n",
       "      <td>0.387755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464395</td>\n",
       "      <td>0.222954</td>\n",
       "      <td>188.638145</td>\n",
       "      <td>0.402810304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443894</td>\n",
       "      <td>0.228838</td>\n",
       "      <td>186.861792</td>\n",
       "      <td>0.466933868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457592</td>\n",
       "      <td>0.271272</td>\n",
       "      <td>190.783934</td>\n",
       "      <td>0.422651934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.448804</td>\n",
       "      <td>0.240951</td>\n",
       "      <td>188.136201</td>\n",
       "      <td>0.378125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f_energy_fat  f_energy_sugar  energy_tot prevalence_overweight_y6\n",
       "0      0.456400        0.234428  187.114757              0.387755102\n",
       "1      0.464395        0.222954  188.638145              0.402810304\n",
       "2      0.443894        0.228838  186.861792              0.466933868\n",
       "3      0.457592        0.271272  190.783934              0.422651934\n",
       "4      0.448804        0.240951  188.136201                 0.378125"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = joined_df[[\"f_energy_fat\", \"f_energy_sugar\", \"energy_tot\", \"prevalence_overweight_y6\"]]\n",
    "data_2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitazing NA data. As they are considered as string in the original dataset, we have to remove them as string, then convert it to float.\n",
    "data_2 = data_2[data_2[\"prevalence_overweight_y6\"] != \"na\"]\n",
    "\n",
    "X_2 = data_2[[\"f_energy_fat\", \"f_energy_sugar\", \"energy_tot\"]]\n",
    "Y_2 = data_2[\"prevalence_overweight_y6\"].astype(float)\n",
    "\n",
    "X_2 = (X_2 - np.mean(X_2, axis=0)) / np.std(X_2, axis=0)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, Y_2, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying ML algorithms \n",
    "\n",
    "Now that we have processed our data, let's run our ML algorithms on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.004\n",
      "R2 score: -0.061\n",
      "coefs [-0.01059494 -0.00627236  0.00633608]\n",
      "A ridge model obtain the best r2 score when the lasso alpha is 9.9. The r2 score is -0.058 and the related mse score is 0.004\n",
      "SVM: Mean squared error: 0.004\n",
      "SVM: R2 score: -0.276\n",
      "Decision Tree: Mean squared error: 0.007\n",
      "Decision Tree: R2 score: -1.138\n",
      "Gradient Boosting Regressor: Mean squared error: 0.004\n",
      "Gradient Boosting Regressor::R2 score: -0.259\n",
      "ADA: Mean squared error: 0.004\n",
      "ADA: R2 score: -0.075\n"
     ]
    }
   ],
   "source": [
    "linear_regression(X_train_2, y_train_2, X_test_2, y_test_2)\n",
    "ridge_model(X_train_2, y_train_2, X_test_2, y_test_2)\n",
    "svm_model(X_train_2, y_train_2, X_test_2, y_test_2)\n",
    "tree_model(X_train_2, y_train_2, X_test_2, y_test_2)\n",
    "GBR_model(X_train_2, y_train_2, X_test_2, y_test_2)\n",
    "ADA_model(X_train_2, y_train_2, X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the result \n",
    "\n",
    "Even though after trying with relative values instead of absolute ones, the model still poorly fit on our dataset.\n",
    "\n",
    "It means that there is little but no correlation between on one side the relative or absolute values of fat / sugar and the total amount of energy of the average product bought at Tesco Grocery, and on the other side the 6 years old children overweight prevalence.\n",
    "\n",
    "Even though we can be disappointed because we wanted to point out a link that doesn't seem to exist, let's try to find out the different reasons that might explain this lack of correlation.\n",
    "\n",
    "To this end, I will use the different results of the Tesco paper figure 5 (the spearman correlation between nutrients and health problems), and the food-related informations of the NHS for [Type-2 diabetes] (https://www.nhs.uk/conditions/type-2-diabetes/food-and-keeping-active/) and [obesity] (https://www.nhs.uk/conditions/obesity/causes/)\n",
    "\n",
    "+ We can note that there is no statistically significant correlations between overweight children (year 6) and energy, fat and sugar. \n",
    "+ We can note that NHS warn about the amount of calories eaten for obesity, but not for type 2 diabetes.\n",
    "+ This result might be due to the fact that physical activity is more important to prevent obesity than type-2 diabetes, which will be more linked to the alimentation. Indeed, we gain weight when the amount of calories burned (which increases with physical activity) is less than the amount of calories ingested. On the other hand, it seems that diet and [especially glucose](https://www.mayoclinic.org/diseases-conditions/type-2-diabetes/symptoms-causes/syc-20351193) play a decisve role to have type 2 diabetes. Hence, the fact that we don't have the physical activity in our study might be more dommageable for the obesity risk than for diabetes.\n",
    "+ Even though more and more children begin to have type-2 diabetes, this medical condition is often detected when the patients are adults. It means that they have kept their eating habits in a period of time way more long, which means that their eating habits have \"beaten\" their natural health conditions, which might be not the case for children (some babies are born with more weight than other naturally)\n",
    "+ As obesity is due to have more calories ingested than burned, the total energy of the \"average product\" might not be that representative, as we don't know how much \"average product\" will be eaten by the children\n",
    "+ As they are children, it is the parents that decide what the family will buy at the grocery store. But the children might eat differently than their parents, but we are not able to know it with our dataset. For example, we don't know if it's the children, or the parents that will eat the sweets.\n",
    "\n",
    "## Third try: using food category\n",
    "\n",
    "Now that we have shown that there is little but no correlations between the nutrients of the average product bought at Tesco and children overweight prevalence at 6 y.o., we will still try to extract a link between the Tesco dataset and the children obesity one.\n",
    "\n",
    "On this try, we will try to create a link between the food categories and the children obesity. From the NHS guidelines, it makes sense that some categories like treated food or sweets will influence children overweight\n",
    "\n",
    "### Data processing\n",
    "\n",
    "Let's process the data, by firstly selecting the corresponding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f_beer',\n",
       " 'f_dairy',\n",
       " 'f_eggs',\n",
       " 'f_fats_oils',\n",
       " 'f_fish',\n",
       " 'f_fruit_veg',\n",
       " 'f_grains',\n",
       " 'f_meat_red',\n",
       " 'f_poultry',\n",
       " 'f_readymade',\n",
       " 'f_sauces',\n",
       " 'f_soft_drinks',\n",
       " 'f_spirits',\n",
       " 'f_sweets',\n",
       " 'f_tea_coffee',\n",
       " 'f_water',\n",
       " 'f_wine']"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_categories = [a for a in joined_df if (a.startswith(\"f_\") and not ('energy' in a) and not('weight' in a))]\n",
    "columns_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it seems unlikely that 6 y.o. children drink wine, beer or spirits, we'll drop those columns.\n",
    "\n",
    "columns_categories.remove('f_beer')\n",
    "columns_categories.remove('f_spirits')\n",
    "columns_categories.remove('f_wine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we'll process the data as usual. Select the columns from the original dataset, filter the valid data, split the data into predictors and targets,\n",
    "# standardize the data, and split it into train and test set.\n",
    "\n",
    "columns_categories.append(\"prevalence_overweight_y6\")\n",
    "\n",
    "data_category = joined_df[columns_categories]\n",
    "data_category = data_category[data_category[\"prevalence_overweight_y6\"] != \"na\"]\n",
    "\n",
    "X_category = data_category[columns_categories[:-1]]\n",
    "Y_category = data_category[\"prevalence_overweight_y6\"].astype(float)\n",
    "\n",
    "X_category = (X_category - np.mean(X_category, axis=0)) / np.std(X_category, axis=0)\n",
    "\n",
    "X_train_category, X_test_category, y_train_category, y_test_category = train_test_split(X_category, Y_category, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning models\n",
    "\n",
    "Now that we have preprocess our data, we'll apply our ML algorithms on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.002\n",
      "R2 score: 0.334\n",
      "coefs [ 0.02897543  0.00719029 -0.01684575  0.02480146  0.02697712  0.01807109\n",
      "  0.02448877  0.00785403  0.0064194   0.00201866  0.02852251  0.0318379\n",
      " -0.0029601   0.03510894]\n",
      "A ridge model obtain the best r2 score when the lasso alpha is 9.9. The r2 score is 0.364 and the related mse score is 0.002\n",
      "SVM: Mean squared error: 0.003\n",
      "SVM: R2 score: 0.128\n",
      "Decision Tree: Mean squared error: 0.005\n",
      "Decision Tree: R2 score: -0.294\n",
      "Gradient Boosting Regressor: Mean squared error: 0.003\n",
      "Gradient Boosting Regressor::R2 score: 0.231\n",
      "ADA: Mean squared error: 0.003\n",
      "ADA: R2 score: 0.231\n"
     ]
    }
   ],
   "source": [
    "linear_regression(X_train_category, y_train_category, X_test_category, y_test_category)\n",
    "ridge_model(X_train_category, y_train_category, X_test_category, y_test_category)\n",
    "svm_model(X_train_category, y_train_category, X_test_category, y_test_category)\n",
    "tree_model(X_train_category, y_train_category, X_test_category, y_test_category)\n",
    "GBR_model(X_train_category, y_train_category, X_test_category, y_test_category)\n",
    "ADA_model(X_train_category, y_train_category, X_test_category, y_test_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Finally, we found a model that fit wells the data. The R2 score for the linear regression is 0.334, which means that the model fits the data better than if we draw the average line.\n",
    "\n",
    "Thus, there is a relation between the type of aliments one's buy at a grocery store and the children overweight prevalence. Let's try to dig it further in a linear regression analysis to analysze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               OLS Regression Results                               \n",
      "====================================================================================\n",
      "Dep. Variable:     prevalence_overweight_y6   R-squared:                       0.486\n",
      "Model:                                  OLS   Adj. R-squared:                  0.474\n",
      "Method:                       Least Squares   F-statistic:                     41.17\n",
      "Date:                      Fri, 18 Dec 2020   Prob (F-statistic):           8.87e-68\n",
      "Time:                              18:23:32   Log-Likelihood:                 899.15\n",
      "No. Observations:                       536   AIC:                            -1772.\n",
      "Df Residuals:                           523   BIC:                            -1717.\n",
      "Df Model:                                12                                         \n",
      "Covariance Type:                  nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        -0.1781      0.274     -0.650      0.516      -0.717       0.360\n",
      "f_dairy           1.1197      0.413      2.711      0.007       0.308       1.931\n",
      "f_eggs            1.8006      1.429      1.260      0.208      -1.006       4.607\n",
      "f_fats_oils      -1.2020      0.774     -1.554      0.121      -2.722       0.318\n",
      "f_fish            5.5408      0.914      6.063      0.000       3.746       7.336\n",
      "f_fruit_veg      -0.1877      0.304     -0.618      0.537      -0.785       0.409\n",
      "f_grains          0.9537      0.357      2.670      0.008       0.252       1.655\n",
      "f_meat_red        1.2509      0.613      2.041      0.042       0.047       2.455\n",
      "f_poultry         3.5036      1.176      2.979      0.003       1.193       5.814\n",
      "f_readymade      -1.7363      0.448     -3.879      0.000      -2.616      -0.857\n",
      "f_sauces         -3.0697      1.307     -2.349      0.019      -5.637      -0.503\n",
      "f_soft_drinks     3.8602      0.576      6.704      0.000       2.729       4.991\n",
      "f_sweets          0.8364      0.311      2.693      0.007       0.226       1.447\n",
      "==============================================================================\n",
      "Omnibus:                        0.998   Durbin-Watson:                   1.820\n",
      "Prob(Omnibus):                  0.607   Jarque-Bera (JB):                0.812\n",
      "Skew:                          -0.069   Prob(JB):                        0.666\n",
      "Kurtosis:                       3.132   Cond. No.                         867.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "df_1 = joined_df[joined_df[\"prevalence_overweight_y6\"] != \"na\"].copy()\n",
    "df_1[\"prevalence_overweight_y6\"] = df_1[\"prevalence_overweight_y6\"].astype(float)\n",
    "mod = smf.ols(formula='prevalence_overweight_y6 ~  f_dairy + f_eggs + f_fats_oils + f_fish + f_fruit_veg + f_grains + f_meat_red + f_poultry + f_readymade + f_sauces + f_soft_drinks + f_sweets', data= df_1)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "# coefficients\n",
    "variables_1 = res.params.index\n",
    "\n",
    "coefficients_1 = res.params.values\n",
    "\n",
    "# p-values\n",
    "p_values_1 = res.pvalues\n",
    "\n",
    "# standard errors\n",
    "standard_errors_1 = res.bse.values\n",
    "stats_1 = pd.DataFrame({'variables_1': variables_1, 'coef_1': coefficients_1, 'p_values_1': p_values_1, 'std_err_1': standard_errors_1})\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables_1</th>\n",
       "      <th>coef_1</th>\n",
       "      <th>p_values_1</th>\n",
       "      <th>std_err_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f_dairy</th>\n",
       "      <td>f_dairy</td>\n",
       "      <td>1.119697</td>\n",
       "      <td>6.934815e-03</td>\n",
       "      <td>0.413066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_fish</th>\n",
       "      <td>f_fish</td>\n",
       "      <td>5.540837</td>\n",
       "      <td>2.558151e-09</td>\n",
       "      <td>0.913846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_grains</th>\n",
       "      <td>f_grains</td>\n",
       "      <td>0.953691</td>\n",
       "      <td>7.819929e-03</td>\n",
       "      <td>0.357181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_meat_red</th>\n",
       "      <td>f_meat_red</td>\n",
       "      <td>1.250866</td>\n",
       "      <td>4.179629e-02</td>\n",
       "      <td>0.613002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_poultry</th>\n",
       "      <td>f_poultry</td>\n",
       "      <td>3.503602</td>\n",
       "      <td>3.023843e-03</td>\n",
       "      <td>1.176003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_readymade</th>\n",
       "      <td>f_readymade</td>\n",
       "      <td>-1.736323</td>\n",
       "      <td>1.181007e-04</td>\n",
       "      <td>0.447581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_sauces</th>\n",
       "      <td>f_sauces</td>\n",
       "      <td>-3.069718</td>\n",
       "      <td>1.917792e-02</td>\n",
       "      <td>1.306632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_soft_drinks</th>\n",
       "      <td>f_soft_drinks</td>\n",
       "      <td>3.860187</td>\n",
       "      <td>5.274776e-11</td>\n",
       "      <td>0.575825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_sweets</th>\n",
       "      <td>f_sweets</td>\n",
       "      <td>0.836434</td>\n",
       "      <td>7.312735e-03</td>\n",
       "      <td>0.310619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 variables_1    coef_1    p_values_1  std_err_1\n",
       "f_dairy              f_dairy  1.119697  6.934815e-03   0.413066\n",
       "f_fish                f_fish  5.540837  2.558151e-09   0.913846\n",
       "f_grains            f_grains  0.953691  7.819929e-03   0.357181\n",
       "f_meat_red        f_meat_red  1.250866  4.179629e-02   0.613002\n",
       "f_poultry          f_poultry  3.503602  3.023843e-03   1.176003\n",
       "f_readymade      f_readymade -1.736323  1.181007e-04   0.447581\n",
       "f_sauces            f_sauces -3.069718  1.917792e-02   1.306632\n",
       "f_soft_drinks  f_soft_drinks  3.860187  5.274776e-11   0.575825\n",
       "f_sweets            f_sweets  0.836434  7.312735e-03   0.310619"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_1[stats_1[\"p_values_1\"] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients_1[1:], variables_1[1:], standard_errors_1[1:], p_values_1[1:])))\n",
    "\n",
    "#fancy plotting\n",
    "\n",
    "plt.errorbar(l1, np.array(range(len(l1))), xerr= 2*np.array(l3), linewidth = 1,\n",
    "             linestyle = 'none',marker = 'o',markersize= 3,\n",
    "             markerfacecolor = 'black',markeredgecolor = 'black', capsize= 5)\n",
    "\n",
    "plt.vlines(0,0, len(l1), linestyle = '--')\n",
    "plt.yticks(range(len(l2)),l2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Chloropleths maps \n",
    "\n",
    "After this Machine Learning part, we want to try to focus on other aspects of our data. Indeed, we have at our disposal a huge dataset containing the food purchases made in the Tesco shops within the boundaries of London. \n",
    "We have many features available for the purchases . We notice that the Tesco paper did not deeply explore the different **food categories**. Therefore, we decide to focus on these ones. \n",
    "\n",
    "We wonder if the Tesco consumers buy the same category products and in same quantity all over the city. We will try to make maps to answer this questions. In particular, we will make maps of the distribution of a certain food category all over London. We consider a mean of the fraction of purchase of each food category in a given ward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special imports for this parts\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from bokeh.io import output_notebook, show, output_file, curdoc,save, export_png\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Slider, HoverTool, GeoJSONDataSource, LinearColorMapper, ColorBar, Dropdown, Select\n",
    "from bokeh.layouts import widgetbox, row, column, gridplot\n",
    "from bokeh.palettes import brewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the purchases data\n",
    "df = grocery.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geographical coordinates of London\n",
    "fp = Path.cwd() / \"data\" / \"London-wards-2018/London-wards-2018_ESRI/London_Ward.shp\"\n",
    "gdf = gpd.read_file(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the income data\n",
    "income_ward = pd.read_csv(Path.cwd() / \"data\" / \"modelled-household-income-estimates-wards.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename certains columns for the vizualition to be more understandable\n",
    "df = df.rename(columns={'f_beer':'beer','f_dairy':'dairy','f_eggs':'eggs','f_fats_oils':'fats_oils','f_fish':'fish',\n",
    "                   'f_fruit_veg':'fruit_veg','f_grains':'grains','f_meat_red':'meat_red','f_poultry':'poultry',\n",
    "                   'f_readymade':'readymade','f_sauces':'sauces','f_soft_drinks':'soft_drinks','f_spirits':'spirits',\n",
    "                   'f_sweets':'sweets','f_tea_coffee':'tea_coffee','f_water':'water','f_wine':'wine'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clean Income values data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_income_value(income):\n",
    "    \"\"\" Transforms the string 'Â£XX,XXX' into the corresponding integer value XXXXX, if an integer value is given returns it\"\"\"\n",
    "    return int(income[1:].replace(',','')) if type(income) != int else income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Dataframe so we can use the values \n",
    "years = ['01/02', '02/03', '03/04', '04/05', '05/06', '06/07', '07/08', '08/09', '09/10', '10/11', '11/12', '12/13']\n",
    "for year in years :\n",
    "    mean_column = 'Mean 20' + year\n",
    "    median_column = 'Median 20' + year\n",
    "    \n",
    "    # Ward\n",
    "    income_ward[mean_column] = income_ward[mean_column].map(lambda i : clean_income_value(i) )\n",
    "    income_ward[median_column] = income_ward[median_column].map(lambda i : clean_income_value(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep data for features of interest\n",
    "features_of_interest = ['Code', 'Mean 2012/13', 'Median 2012/13']\n",
    "\n",
    "income_ward_12_13 = income_ward[features_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Mean_2012</th>\n",
       "      <th>Median_2012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>99390</td>\n",
       "      <td>63620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E05000026</td>\n",
       "      <td>38870</td>\n",
       "      <td>33920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E05000027</td>\n",
       "      <td>37290</td>\n",
       "      <td>32470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05000028</td>\n",
       "      <td>37860</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E05000029</td>\n",
       "      <td>38860</td>\n",
       "      <td>33920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Code  Mean_2012  Median_2012\n",
       "0  E09000001      99390        63620\n",
       "1  E05000026      38870        33920\n",
       "2  E05000027      37290        32470\n",
       "3  E05000028      37860        33000\n",
       "4  E05000029      38860        33920"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns for them to be understand when doing the chloropleth maps\n",
    "income_ward_12_13 = income_ward_12_13.rename(columns={'Mean 2012/13':'Mean_2012','Median 2012/13':'Median_2012'})\n",
    "income_ward_12_13.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the purchases and income dataframe with the geographical coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the purchases dataframe with the geographical coordinates on the GSS_code (left join to preserve all the gdf rows)\n",
    "join_df = gdf.join(df.set_index('area_id'), on = 'GSS_CODE', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSS_CODE</th>\n",
       "      <th>DISTRICT</th>\n",
       "      <th>LAGSSCODE</th>\n",
       "      <th>HECTARES</th>\n",
       "      <th>NONLD_AREA</th>\n",
       "      <th>geometry</th>\n",
       "      <th>weight</th>\n",
       "      <th>weight_perc2.5</th>\n",
       "      <th>weight_perc25</th>\n",
       "      <th>...</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>age_0_17</th>\n",
       "      <th>age_18_64</th>\n",
       "      <th>age_65+</th>\n",
       "      <th>avg_age</th>\n",
       "      <th>area_sq_km</th>\n",
       "      <th>people_per_sq_km</th>\n",
       "      <th>Mean_2012</th>\n",
       "      <th>Median_2012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Coulsdon Town</td>\n",
       "      <td>E05011466</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>E09000008</td>\n",
       "      <td>452.138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((528552.301 159658.098, 528556.003 15...</td>\n",
       "      <td>311.240543</td>\n",
       "      <td>30.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6100.0</td>\n",
       "      <td>6346.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>7496.0</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>40.196288</td>\n",
       "      <td>5.61</td>\n",
       "      <td>2218.538324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Purley &amp; Woodcote</td>\n",
       "      <td>E05011476</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>E09000008</td>\n",
       "      <td>509.713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((529397.601 160698.600, 529328.697 16...</td>\n",
       "      <td>302.463394</td>\n",
       "      <td>30.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9201.0</td>\n",
       "      <td>9654.0</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>11603.0</td>\n",
       "      <td>3222.0</td>\n",
       "      <td>40.223230</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3101.151316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Waddon</td>\n",
       "      <td>E05011487</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>E09000008</td>\n",
       "      <td>391.981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((531218.597 162415.198, 531223.099 16...</td>\n",
       "      <td>323.975163</td>\n",
       "      <td>30.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7594.0</td>\n",
       "      <td>7998.0</td>\n",
       "      <td>4160.0</td>\n",
       "      <td>9750.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>35.029374</td>\n",
       "      <td>2.62</td>\n",
       "      <td>5951.145038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Old Coulsdon</td>\n",
       "      <td>E05011474</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>E09000008</td>\n",
       "      <td>693.502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((532745.096 157404.603, 532768.100 15...</td>\n",
       "      <td>287.540428</td>\n",
       "      <td>28.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4396.0</td>\n",
       "      <td>4836.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>5266.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>42.378791</td>\n",
       "      <td>5.35</td>\n",
       "      <td>1725.607477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kenley</td>\n",
       "      <td>E05011469</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>E09000008</td>\n",
       "      <td>519.683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((533742.699 159283.905, 533721.196 15...</td>\n",
       "      <td>300.385988</td>\n",
       "      <td>29.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5264.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>6184.0</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>39.820733</td>\n",
       "      <td>5.07</td>\n",
       "      <td>2024.457594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>E05009289</td>\n",
       "      <td>City and County of the City of London</td>\n",
       "      <td>E09000001</td>\n",
       "      <td>11.230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((533315.597 180988.802, 533305.703 18...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Broad Street</td>\n",
       "      <td>E05009295</td>\n",
       "      <td>City and County of the City of London</td>\n",
       "      <td>E09000001</td>\n",
       "      <td>8.157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((533005.698 181612.203, 533099.098 18...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>Coleman Street</td>\n",
       "      <td>E05009299</td>\n",
       "      <td>City and County of the City of London</td>\n",
       "      <td>E09000001</td>\n",
       "      <td>15.061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((532552.797 181289.396, 532554.198 18...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>Portsoken</td>\n",
       "      <td>E05009308</td>\n",
       "      <td>City and County of the City of London</td>\n",
       "      <td>E09000001</td>\n",
       "      <td>6.288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((533814.299 180968.898, 533757.202 18...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>Candlewick</td>\n",
       "      <td>E05009296</td>\n",
       "      <td>City and County of the City of London</td>\n",
       "      <td>E09000001</td>\n",
       "      <td>5.209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((532681.098 180898.198, 532687.496 18...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows Ã— 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  NAME   GSS_CODE                               DISTRICT  \\\n",
       "16       Coulsdon Town  E05011466                                Croydon   \n",
       "17   Purley & Woodcote  E05011476                                Croydon   \n",
       "18              Waddon  E05011487                                Croydon   \n",
       "19        Old Coulsdon  E05011474                                Croydon   \n",
       "20              Kenley  E05011469                                Croydon   \n",
       "..                 ...        ...                                    ...   \n",
       "652            Aldgate  E05009289  City and County of the City of London   \n",
       "653       Broad Street  E05009295  City and County of the City of London   \n",
       "654     Coleman Street  E05009299  City and County of the City of London   \n",
       "655          Portsoken  E05009308  City and County of the City of London   \n",
       "656         Candlewick  E05009296  City and County of the City of London   \n",
       "\n",
       "     LAGSSCODE  HECTARES  NONLD_AREA  \\\n",
       "16   E09000008   452.138         0.0   \n",
       "17   E09000008   509.713         0.0   \n",
       "18   E09000008   391.981         0.0   \n",
       "19   E09000008   693.502         0.0   \n",
       "20   E09000008   519.683         0.0   \n",
       "..         ...       ...         ...   \n",
       "652  E09000001    11.230         0.0   \n",
       "653  E09000001     8.157         0.0   \n",
       "654  E09000001    15.061         0.0   \n",
       "655  E09000001     6.288         0.0   \n",
       "656  E09000001     5.209         0.0   \n",
       "\n",
       "                                              geometry      weight  \\\n",
       "16   POLYGON ((528552.301 159658.098, 528556.003 15...  311.240543   \n",
       "17   POLYGON ((529397.601 160698.600, 529328.697 16...  302.463394   \n",
       "18   POLYGON ((531218.597 162415.198, 531223.099 16...  323.975163   \n",
       "19   POLYGON ((532745.096 157404.603, 532768.100 15...  287.540428   \n",
       "20   POLYGON ((533742.699 159283.905, 533721.196 15...  300.385988   \n",
       "..                                                 ...         ...   \n",
       "652  POLYGON ((533315.597 180988.802, 533305.703 18...         NaN   \n",
       "653  POLYGON ((533005.698 181612.203, 533099.098 18...         NaN   \n",
       "654  POLYGON ((532552.797 181289.396, 532554.198 18...         NaN   \n",
       "655  POLYGON ((533814.299 180968.898, 533757.202 18...         NaN   \n",
       "656  POLYGON ((532681.098 180898.198, 532687.496 18...         NaN   \n",
       "\n",
       "     weight_perc2.5  weight_perc25  ...    male  female  age_0_17  age_18_64  \\\n",
       "16             30.0          125.0  ...  6100.0  6346.0    2800.0     7496.0   \n",
       "17             30.0          125.0  ...  9201.0  9654.0    4030.0    11603.0   \n",
       "18             30.0          135.0  ...  7594.0  7998.0    4160.0     9750.0   \n",
       "19             28.0          112.0  ...  4396.0  4836.0    2005.0     5266.0   \n",
       "20             29.0          125.0  ...  5000.0  5264.0    2360.0     6184.0   \n",
       "..              ...            ...  ...     ...     ...       ...        ...   \n",
       "652             NaN            NaN  ...     NaN     NaN       NaN        NaN   \n",
       "653             NaN            NaN  ...     NaN     NaN       NaN        NaN   \n",
       "654             NaN            NaN  ...     NaN     NaN       NaN        NaN   \n",
       "655             NaN            NaN  ...     NaN     NaN       NaN        NaN   \n",
       "656             NaN            NaN  ...     NaN     NaN       NaN        NaN   \n",
       "\n",
       "     age_65+    avg_age  area_sq_km  people_per_sq_km  Mean_2012  Median_2012  \n",
       "16    2150.0  40.196288        5.61       2218.538324        NaN          NaN  \n",
       "17    3222.0  40.223230        6.08       3101.151316        NaN          NaN  \n",
       "18    1682.0  35.029374        2.62       5951.145038        NaN          NaN  \n",
       "19    1961.0  42.378791        5.35       1725.607477        NaN          NaN  \n",
       "20    1720.0  39.820733        5.07       2024.457594        NaN          NaN  \n",
       "..       ...        ...         ...               ...        ...          ...  \n",
       "652      NaN        NaN         NaN               NaN        NaN          NaN  \n",
       "653      NaN        NaN         NaN               NaN        NaN          NaN  \n",
       "654      NaN        NaN         NaN               NaN        NaN          NaN  \n",
       "655      NaN        NaN         NaN               NaN        NaN          NaN  \n",
       "656      NaN        NaN         NaN               NaN        NaN          NaN  \n",
       "\n",
       "[174 rows x 210 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the income dataframe with the geographical coordinates on the GSS_code (left join to preserve all the gdf rows)\n",
    "join_df = join_df.join(income_ward_12_13.set_index('Code'), on = 'GSS_CODE', how = 'left')\n",
    "join_df.head(5)\n",
    "null_data = join_df[join_df.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "fillna currently only supports filling with a scalar geometry",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-678150089a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Replace NaN values with the string 'No data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjoin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'No data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ADA/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   4312\u001b[0m         \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     ) -> Optional[\"DataFrame\"]:\n\u001b[0;32m-> 4314\u001b[0;31m         return super().fillna(\n\u001b[0m\u001b[1;32m   4315\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4316\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ADA/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6069\u001b[0;31m                 new_data = self._mgr.fillna(\n\u001b[0m\u001b[1;32m   6070\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6071\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/envs/ADA/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"BlockManager\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0;34m\"fillna\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/ADA/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ADA/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m         return [\n\u001b[1;32m   1775\u001b[0m             self.make_block_same_class(\n",
      "\u001b[0;32m~/miniconda3/envs/ADA/lib/python3.8/site-packages/geopandas/array.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseGeometry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m             raise NotImplementedError(\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;34m\"fillna currently only supports filling with a scalar geometry\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: fillna currently only supports filling with a scalar geometry"
     ]
    }
   ],
   "source": [
    "# Replace NaN values with the string 'No data'\n",
    "join_df = join_df.fillna(value='No data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chloropleth maps helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_plot(json_data):\n",
    "    '''begin a plot by initializing the geosource and the palette of colors used'''\n",
    "    geosource = GeoJSONDataSource(geojson = json_data)\n",
    "    palette = brewer['YlGnBu'][9]\n",
    "    palette = palette[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(item):\n",
    "    '''make a chloropleth map for a certain item (food category in our case)'''\n",
    "    \n",
    "    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n",
    "    min_ = min(list(filter(lambda a : a != 'No data', join_df[item].values)))\n",
    "    max_ = max(list(filter(lambda a : a != 'No data', join_df[item].values)))\n",
    "    \n",
    "    color_mapper = LinearColorMapper(palette = palette, low = min_, high = max_, nan_color = '#d9d9d9')\n",
    "    \n",
    "    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20, border_line_color=None,location = (0,0), orientation = 'horizontal')\n",
    "    hover = HoverTool(tooltips = [('Region','@NAME'),('% ' + item, '@' + item),('Mean outcome ' ,'$@Mean_2012{,}'),('Median outcome ','$@Median_2012{,}')])\n",
    "\n",
    "    p = figure(title = 'Fraction of {} in basket'.format(item), plot_height = 400 , plot_width = 470, toolbar_location = None, tools = [hover])\n",
    "    p.title.text_font = 'helvetica'\n",
    "    p.title.text_font_style = 'bold'\n",
    "    p.title.text_font_size = '16px'\n",
    "    p.title.align = 'center'\n",
    "    \n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    \n",
    "    #Add patch renderer to figure. \n",
    "    p.patches('xs','ys', source = geosource,fill_color = {'field' :item, 'transform' : color_mapper},\n",
    "              line_color = 'black', line_width = 0.25, fill_alpha = 1)\n",
    "    \n",
    "    #Specify figure layout.\n",
    "    p.add_layout(color_bar, 'below')\n",
    "    \n",
    "    #Display figure inline in Jupyter Notebook.\n",
    "    p.xaxis.visible = False\n",
    "    p.yaxis.visible = False\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(attr, old, new):\n",
    "    '''update the plot for the case of an interactive map with a selecter'''\n",
    "    # The input item is the category selected from the select box\n",
    "    item = select.value\n",
    "    \n",
    "    # Update the plot based on the changed inputs\n",
    "    p = make_plot(item)\n",
    "    \n",
    "    # Update the layout, clear the old document and display the new document\n",
    "    layout = column(p, widgetbox(select))\n",
    "    curdoc().clear()\n",
    "    curdoc().add_root(layout)\n",
    "    geosource.geojson = json_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chloropleth Maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make an interactive map with a selector with represent the fraction of purchases of all food categories in London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To see it, run the next cell, open a terminal and write \"bokeh serve --show InteractiveMap.ipynb\", you should see the interactive map on a html page and you could change the category of food selected. It needs that you have the last version of geopandas installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data to json.\n",
    "merged_json = json.loads(join_df.to_json())\n",
    "\n",
    "# Convert to String like object.\n",
    "json_data = json.dumps(merged_json)\n",
    "\n",
    "#Input geojson source and initial category depicted\n",
    "geosource = GeoJSONDataSource(geojson = json_data)\n",
    "input_field = 'beer'\n",
    "palette = brewer['YlGnBu'][9]\n",
    "\n",
    "#Reverse color order so that dark blue is highest fraction of the category.\n",
    "palette = palette[::-1]\n",
    "    \n",
    "# Call the plotting function\n",
    "p = make_plot(input_field)\n",
    "\n",
    "# Make a selection object: select\n",
    "select = Select(title='Select Categories:', value='beer', options=['beer','dairy','eggs','fats_oils','fish',\n",
    "                                                                     'fruit_veg','grains','meat_red','poultry',\n",
    "                                                                     'readymade','sauces','soft_drinks','spirits',\n",
    "                                                                     'sweets','tea_coffee','water','wine'])\n",
    "select.on_change('value', update_plot)\n",
    "\n",
    "# Make a column layout of widgetbox(select) and plot, and add it to the current document\n",
    "layout = column(p, widgetbox(select))\n",
    "curdoc().add_root(layout)\n",
    "\n",
    "# Display the current document\n",
    "output_notebook()\n",
    "show(layout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make one plot per category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each plot below represents the mean proportion of purchase of a certain food category per ward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_plot(json_data)\n",
    "p_meat_red = make_plot('meat_red')\n",
    "begin_plot(json_data)\n",
    "p_beer = make_plot('beer')\n",
    "begin_plot(json_data)\n",
    "p_fruit_veg = make_plot('fruit_veg')\n",
    "begin_plot(json_data)\n",
    "p_eggs = make_plot('eggs')\n",
    "begin_plot(json_data)\n",
    "p_sweets = make_plot('sweets')\n",
    "begin_plot(json_data)\n",
    "p_poultry = make_plot('poultry')\n",
    "begin_plot(json_data)\n",
    "p_readymade = make_plot('readymade')\n",
    "begin_plot(json_data)\n",
    "p_soft_drinks = make_plot('soft_drinks')\n",
    "begin_plot(json_data)\n",
    "p_spirits = make_plot('spirits')\n",
    "begin_plot(json_data)\n",
    "p_sauces = make_plot('sauces')\n",
    "begin_plot(json_data)\n",
    "p_grains = make_plot('grains')\n",
    "begin_plot(json_data)\n",
    "p_tea_coffee = make_plot('tea_coffee')\n",
    "begin_plot(json_data)\n",
    "p_fish = make_plot('fish')\n",
    "begin_plot(json_data)\n",
    "p_fat_oils = make_plot('fats_oils')\n",
    "begin_plot(json_data)\n",
    "p_water = make_plot('water')\n",
    "begin_plot(json_data)\n",
    "p_wine = make_plot('wine')\n",
    "begin_plot(json_data)\n",
    "p_dairy = make_plot('dairy')\n",
    "\n",
    "layout = gridplot([[p_meat_red,p_beer,p_fruit_veg, p_eggs], [p_sweets, p_poultry,p_readymade, p_soft_drinks],\n",
    "                  [p_spirits,p_sauces,p_grains,p_tea_coffee],[p_fish,p_fat_oils,p_water,p_wine],[p_dairy]])\n",
    "curdoc().add_root(layout)\n",
    "output_notebook()\n",
    "show(layout)\n",
    "\n",
    "# Next useful when you want to save the file\n",
    "#output_file(\"maps.html\") \n",
    "#save(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Observations : We see that the distributions of proportion of purchases of food category are pretty different. Some food categories shows uniform distributions whereas some others show very disparate distributions. For instance, the comsuption of beer, soft drinks and tea/coffee seem to be the same all over the city. In contrary, the consumption of meat is 6 times higher in the northern and centered wards of London compared to some southern ward. We observe almost the same thing for the poultry. The distribution of ready made, sweets and grains seem to be more important in the southern, eastern and western edge of the city. It is particularly true for ready made where the consumption of this category is almost 50 time higher between some edge parts and some city centered parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make plots for the mean income and the median income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot for the income\n",
    "def make_plot_income(item):\n",
    "    '''make a chloropleth map for a certain item (food category in our case)'''\n",
    "    \n",
    "    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n",
    "    min_ = min(list(filter(lambda a : a != 'No data', join_df[item].values)))\n",
    "    max_ = max(list(filter(lambda a : a != 'No data', join_df[item].values)))\n",
    "    \n",
    "    color_mapper = LinearColorMapper(palette = palette, low = min_, high = max_, nan_color = '#d9d9d9')\n",
    "    \n",
    "    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20, border_line_color=None,location = (0,0), orientation = 'horizontal')\n",
    "    hover = HoverTool(tooltips = [('Region','@NAME'),(item, '$@' + item + '{,}')])\n",
    "\n",
    "    p = figure(title = 'Fraction of {} in basket'.format(item), plot_height = 400 , plot_width = 470, toolbar_location = None, tools = [hover])\n",
    "    p.title.text_font = 'helvetica'\n",
    "    p.title.text_font_style = 'bold'\n",
    "    p.title.text_font_size = '16px'\n",
    "    p.title.align = 'center'\n",
    "    \n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    \n",
    "    #Add patch renderer to figure. \n",
    "    p.patches('xs','ys', source = geosource,fill_color = {'field' :item, 'transform' : color_mapper},\n",
    "              line_color = 'black', line_width = 0.25, fill_alpha = 1)\n",
    "    \n",
    "    #Specify figure layout.\n",
    "    p.add_layout(color_bar, 'below')\n",
    "    \n",
    "    #Display figure inline in Jupyter Notebook.\n",
    "    p.xaxis.visible = False\n",
    "    p.yaxis.visible = False\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_plot(json_data)\n",
    "p_mean = make_plot_income('Mean_2012')\n",
    "begin_plot(json_data)\n",
    "p_median = make_plot_income('Median_2012')\n",
    "layout = gridplot([[p_mean,p_median]])\n",
    "curdoc().add_root(layout)\n",
    "output_notebook()\n",
    "show(layout)\n",
    "#Next useful to save\n",
    "#output_file(\"income.html\") \n",
    "#save(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Link between groceries and income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Intrigued by the last observations, we will try to see here if there exist a correlation between the income of people and the proportion of purchases of some food category in the Tesco shops for Ward, MSOA and LSOA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "groceries_ward = grocery.copy()\n",
    "groceries_msoa = pd.read_csv(Path.cwd() / \"data\" / \"year_msoa_grocery.csv\")\n",
    "groceries_lsoa = pd.read_csv(Path.cwd() / \"data\" / \"year_lsoa_grocery.csv\")\n",
    "income_msoa = pd.read_csv(Path.cwd() / \"data\" / \"modelled-household-income-estimates-msoa.csv\", encoding='latin1')\n",
    "income_lsoa = pd.read_csv(Path.cwd() / \"data\" / \"modelled-household-income-estimates-lsoa.csv\", encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have Shopping data for {len(groceries_ward)} Wards, {len(groceries_msoa)} MSOAs and {len(groceries_lsoa)} LSOAs')\n",
    "print(f'We have Income data for {len(income_ward)} Wards, {len(income_msoa)} MSOAs and {len(income_lsoa)} LSOAs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Dataframe so we can use the values \n",
    "for year in years :\n",
    "    mean_column = 'Mean 20' + year\n",
    "    median_column = 'Median 20' + year\n",
    "    # MSOA\n",
    "    income_msoa[mean_column] = income_msoa[mean_column].map(lambda i : clean_income_value(i) )\n",
    "    income_msoa[median_column] = income_msoa[median_column].map(lambda i : clean_income_value(i))\n",
    "    # LSOA\n",
    "    income_lsoa[mean_column] = income_lsoa[mean_column].map(lambda i : clean_income_value(i) )\n",
    "    income_lsoa[median_column] = income_lsoa[median_column].map(lambda i : clean_income_value(i))\n",
    "    \n",
    "income_msoa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = ['Mean 20' + year for year in years]\n",
    "median_columns = ['Median 20' + year for year in years]\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize=(13,13), sharey=True)\n",
    "ward_sample = np.random.choice(range(len(income_ward)), 20, replace=False)\n",
    "msoa_sample = np.random.choice(range(len(income_msoa)), 20, replace=False)\n",
    "lsoa_sample = np.random.choice(range(len(income_lsoa)), 20, replace=False)\n",
    "\n",
    "for i in ward_sample:\n",
    "    axes[0,0].plot(pd.DataFrame(income_ward.iloc[i][mean_columns]))\n",
    "    axes[0,0].set_xticklabels(labels=mean_columns, rotation=90)\n",
    "    axes[0,1].plot(pd.DataFrame(income_ward.iloc[i][median_columns]))\n",
    "    axes[0,1].set_xticklabels(labels=median_columns, rotation=90)\n",
    "    \n",
    "for i in msoa_sample:\n",
    "    axes[1,0].plot(pd.DataFrame(income_msoa.iloc[i][mean_columns]))\n",
    "    axes[1,0].set_xticklabels(labels=mean_columns, rotation=90)\n",
    "    axes[1,1].plot(pd.DataFrame(income_msoa.iloc[i][median_columns]))\n",
    "    axes[1,1].set_xticklabels(labels=median_columns, rotation=90)\n",
    "    \n",
    "for i in lsoa_sample:\n",
    "    axes[2,0].plot(pd.DataFrame(income_lsoa.iloc[i][mean_columns]))\n",
    "    axes[2,0].set_xticklabels(labels=mean_columns, rotation=90)\n",
    "    axes[2,1].plot(pd.DataFrame(income_lsoa.iloc[i][median_columns]))\n",
    "    axes[2,1].set_xticklabels(labels=median_columns, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we are interested in seeing how the Mean and Median income evolves between the years. \n",
    "\n",
    "As we can see on the graph above, there are some outliers but we can see that overall the Mean and Median outcome tend to go up ! \n",
    "\n",
    "We decide to use the revenue for year 2012/13 as it is the closest data we have to 2015. Note that since we see a tendency to going growing, we can assume that an overall increasing trend is to be exected. \n",
    "\n",
    "We could find a regressor to find an expected value for 2014/15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between Groceries propostions and Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep data for features of interest\n",
    "features_of_interest = ['Code', 'Mean 2012/13', 'Median 2012/13']\n",
    "\n",
    "income_ward_12_13 = income_ward[features_of_interest]\n",
    "income_msoa_12_13 = income_msoa[features_of_interest]\n",
    "income_lsoa_12_13 = income_lsoa[features_of_interest]\n",
    "\n",
    "# Join the Income Data on the corresponding Shopping Data \n",
    "\n",
    "groceries_income_ward = groceries_ward.join(income_ward_12_13.set_index('Code'),on='area_id',how='inner').rename(columns={'Mean 2012/13': 'mean_income', 'Median 2012/13': 'median_income'})\n",
    "groceries_income_msoa = groceries_msoa.join(income_msoa_12_13.set_index('Code'),on='area_id',how='inner').rename(columns={'Mean 2012/13': 'mean_income', 'Median 2012/13': 'median_income'})\n",
    "groceries_income_lsoa = groceries_lsoa.join(income_lsoa_12_13.set_index('Code'),on='area_id',how='inner').rename(columns={'Mean 2012/13': 'mean_income', 'Median 2012/13': 'median_income'})\n",
    "\n",
    "groceries_income_msoa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in groceries_income_msoa:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_fraction = ['f_beer', 'f_dairy', 'f_eggs', 'f_fats_oils', 'f_fish', 'f_fruit_veg', 'f_grains', 'f_meat_red', \n",
    "                       'f_poultry', 'f_readymade', 'f_sauces', 'f_soft_drinks', 'f_spirits', 'f_sweets', 'f_tea_coffee', \n",
    "                       'f_water', 'f_wine']\n",
    "\n",
    "nutrients_fraction = ['energy_fat', 'energy_saturate', 'energy_sugar', 'energy_protein',\n",
    "                      'energy_carb', 'energy_fibre', 'energy_alcohol', 'energy_tot']\n",
    "\n",
    "groceries_dfs = [('ward', groceries_income_ward), \n",
    "                 ('msoa', groceries_income_msoa), \n",
    "                 ('lsoa', groceries_income_lsoa)]\n",
    "\n",
    "def compute_correlations_categories_incomes(df, income, categories=categories_fraction):\n",
    "    r_values = []\n",
    "    p_values = []\n",
    "    category = []\n",
    "\n",
    "    for cat in categories:\n",
    "        correl, p_value = stats.spearmanr(df[cat],df[income])\n",
    "        #print(f\"- {cat} : {correl} with p-value {p_value}\")\n",
    "        r_values.append(correl)\n",
    "        p_values.append(p_value)\n",
    "        category.append(cat)\n",
    "\n",
    "\n",
    "    correlation_df = pd.DataFrame({'Category' : category, 'R': r_values, 'p': p_values, 'statistically_significant' : [ p < 0.05 for p in p_values]})\n",
    "    correlation_df\n",
    "    \n",
    "    return correlation_df\n",
    "\n",
    "def compute_correlations_categories_incomes_list(dfs, income, categories=categories_fraction):\n",
    "    correlation_df = pd.DataFrame({'Category' : categories})\n",
    "    for name, df in dfs:\n",
    "        r_values = []\n",
    "        p_values = []\n",
    "\n",
    "        for cat in categories:\n",
    "            correl, p_value = stats.spearmanr(df[cat],df[income])\n",
    "            #print(f\"- {cat} : {correl} with p-value {p_value}\")\n",
    "            r_values.append(correl)\n",
    "            p_values.append(p_value)\n",
    "\n",
    "        correlation_df['R_'+name] = r_values\n",
    "        correlation_df['p_'+name] = p_values\n",
    "        correlation_df['stat_'+name] = [ p < 0.05 for p in p_values]\n",
    "    \n",
    "    return correlation_df\n",
    "\n",
    "def compute_correlations_categories_incomes_grouped(dfs, income, categories=categories_fraction, category_name = 'category'):\n",
    "    r_values = []\n",
    "    p_values = []\n",
    "    census = []\n",
    "    for name, df in dfs:\n",
    "        for cat in categories:\n",
    "            correl, p_value = stats.spearmanr(df[cat],df[income])\n",
    "            #print(f\"- {cat} : {correl} with p-value {p_value}\")\n",
    "            r_values.append(correl)\n",
    "            p_values.append(p_value)\n",
    "            census.append(name)\n",
    "\n",
    "    correlation_df = pd.DataFrame({category_name : categories * len(dfs), 'census': census, 'R': r_values, 'p': p_values, 'stat_sig' : [ p < 0.05 for p in p_values]})\n",
    "\n",
    "    return correlation_df.groupby([category_name,'census']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the correlation between Mean Income in 2012/13 and the food categories ! \n",
    "correlation_all_census = compute_correlations_categories_incomes_grouped(dfs=groceries_dfs, income='mean_income')\n",
    "correlation_all_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories_labels = ['Beer', 'Dairy', 'Eggs', 'Fats Oils', 'Fish', 'Fruit Veg', 'Grains', 'Red meat', \n",
    "                       'Poultry', 'Readymade', 'Sauces', 'Soft drinks', 'Spirits', 'Sweets', 'Tea coffee', \n",
    "                       'Water', 'Wine']\n",
    "\n",
    "correlation_msoa = compute_correlations_categories_incomes(df=groceries_income_msoa, income='mean_income')\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title('MSOA : Correlation of fraction of food category and \\nMean Income ', fontsize=20)\n",
    "plt.bar(correlation_msoa.Category,correlation_msoa.R, color = [ 'white' if not v[3] else'tab:orange' if v[1] > 0 else 'tab:blue' for v in correlation_msoa.values])\n",
    "plt.xticks(correlation_msoa.Category,categories_labels, rotation='vertical',fontsize=15)\n",
    "plt.axhline(y=0,color='black') \n",
    "plt.ylabel('R',fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "correlation_msoa = compute_correlations_categories_incomes(df=groceries_income_msoa, income='median_income')\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title('MSOA : Correlation of fraction of food category and \\nMedian Income', fontsize=20)\n",
    "plt.bar(correlation_msoa.Category,correlation_msoa.R, color = [ 'white' if not v[3] else'tab:orange' if v[1] > 0 else 'tab:blue' for v in correlation_msoa.values])\n",
    "plt.xticks(correlation_msoa.Category,categories_labels, rotation='vertical',fontsize=15)\n",
    "plt.axhline(y=0,color='black') \n",
    "plt.ylabel('R',fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_sts(df, color): \n",
    "    return [ 'white' if not v else color for v in df.statistically_significant.values]\n",
    "\n",
    "correlation_msoa = compute_correlations_categories_incomes(df=groceries_income_msoa, income='mean_income')\n",
    "correlation_lsoa = compute_correlations_categories_incomes(df=groceries_income_lsoa, income='mean_income')\n",
    "correlation_ward = compute_correlations_categories_incomes(df=groceries_income_ward, income='mean_income')\n",
    "\n",
    "print(correlation_msoa.columns)\n",
    "width = 0.2\n",
    "x = np.arange(len(categories_labels))\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title('Correlation of fraction of food category and \\nMean Income', fontsize=20)\n",
    "plt.bar(x - width,correlation_lsoa.R, width=width, color = color_sts(correlation_lsoa,'tab:blue'), label='LSOA')\n",
    "plt.bar(x, correlation_msoa.R, width=width, color = color_sts(correlation_msoa,'tab:orange'), label='MSOA')\n",
    "plt.bar(x + width,correlation_ward.R, width=width, color = color_sts(correlation_ward,'tab:grey'), label='WARD')\n",
    "plt.xticks(x ,categories_labels, rotation='vertical',fontsize=15)\n",
    "plt.axhline(y=0,color='black') \n",
    "plt.ylabel('R',fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here first we compute the correlations between the percentage of each food category and the mean income. Note that we obtain similar values with median(we display data below). We thus can conduct the same analysis on both dataframes and choose to do it on this one.  \n",
    "\n",
    "\n",
    "First it's worth noting what these correlations mean. A positive correlation means that having a greater fraction of all the products belonging to a category relates to a higher Mean Income.\n",
    "So it does not mean that people with high revenues don't buy any product of a negatively correlated category or even that they buy less. It means that the fraction of their shopping bag item belonging to that category is smaller. \n",
    "Taking the example of grains, this category being negatively correlated with income means that people with lower income overall have a bigger part of their alimentation composed of grains. \n",
    "\n",
    "So let's analyse these results. \n",
    "- The negatives correlation of Income with  Sweets, Spirits, Soft Drinks, Fats Oils and Grains is quite interesting because it shows that poorer neighbourhood show a higher part of the groceries dedicated to less healthy food. The correlation with spirits is also quite interesting as it might induce that poorer neighbourhood might be more prone to alcoholism or at least a higher consumption of stronger alcohol. \n",
    "\n",
    "- The positive correlation of Income with Fruit and Vegetables, Fish and Dairy product is also quite interesting as we here find that 'healthier' products take a bigger part of the shopping bag in areas where the income is higher. Fruit and vegetables can also be explained as people with higher incomes might be more prone to be vegetarian. Wine is also one of those positively correlated food categories and we can explain that as wine being a 'fancy drink'. \n",
    "\n",
    "- The fraction of water being negatively correlated with water seems quite surprising at first but we can actually make a point by finding that tap water in poorer neighbouhoods might be of lower quality forcing people there to buy more bottled water. People in richer neighbourhood might also be more likely to buy a water filtering system. It might also be that people might tend to buy big packs of water bottles in huge malls situated in industrial areas where the global Income might be lower and thus correlating it with lower Income.\n",
    "\n",
    "- Fraction of tea and coffee being negatively correlated with income is not very, so is the positive one with beer. \n",
    "\n",
    "- We let aside the correlation that are not statistically significant or very low, namely eggs, poultry, red meat and ready-made food. Although for the latter, we can suppose that any worker is sensible to buying ready-made food for lunch thus it does not really correlates with the Income. \n",
    "\n",
    "Important to qualify these remarks : \n",
    "\n",
    "- The people buying food in a given area don't necessarily live in the area. \n",
    "\n",
    "- It might be that some big shopping center are situated in industrial places outside the city, thus not much people live there so the mean income would be quite low even though all sorts of people go shopping there. \n",
    "\n",
    "- We also can note that areas with really high median incomes might not be the most populated ones (finacial districts ... ) and might also not be the shopping place of a lot of people. Thus this most likely gives skewed data.\n",
    "\n",
    "We now address some of these questions by looking at the relationship between the population of an area and its Mean Income. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population\n",
    "print(stats.spearmanr(groceries_income_msoa['population'],groceries_income_msoa['median_income']))\n",
    "print(stats.spearmanr(groceries_income_msoa['population'],groceries_income_msoa['mean_income']))\n",
    "\n",
    "# People Density \n",
    "print(stats.spearmanr(groceries_income_msoa['people_per_sq_km'],groceries_income_msoa['median_income']))\n",
    "print(stats.spearmanr(groceries_income_msoa['people_per_sq_km'],groceries_income_msoa['mean_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative correlation so areas more populated tend to have a lower mean and median income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_correlations_categories_incomes_grouped(dfs=groceries_dfs, income='median_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for area with representativeness_norm > threshold\n",
    "\n",
    "threshold = 0.3\n",
    "groceries_rep_dfs = [('ward', groceries_income_ward[groceries_income_ward['representativeness_norm'] > threshold]), \n",
    "                 ('msoa', groceries_income_msoa[groceries_income_msoa['representativeness_norm'] > threshold]), \n",
    "                 ('lsoa', groceries_income_lsoa[groceries_income_lsoa['representativeness_norm'] > threshold])]\n",
    "\n",
    "correlation_all_census_rep_02 = compute_correlations_categories_incomes_grouped(dfs=groceries_rep_dfs, income='mean_income')\n",
    "correlation_all_census_rep_02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even if we take the areas where representativeness_norm > 0.3, the correlation stay more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the correlation between Mean Income in 2012/13 and the energy from nutients  ! \n",
    "compute_correlations_categories_incomes_grouped(dfs=groceries_dfs, income='mean_income', categories=nutrients_fraction, category_name='nutrient')\n",
    "correlation_nutrients = compute_correlations_categories_incomes(groceries_income_msoa, income='mean_income', categories=nutrients_fraction)\n",
    "correlation_nutrients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title('MSOA : Correlation of fraction of nutrients and \\nMedian Income', fontsize=20)\n",
    "plt.bar(correlation_nutrients.Category, correlation_nutrients.R, color = [ 'white' if not v[3] else'tab:orange' if v[1] > 0 else 'tab:blue' for v in correlation_nutrients.values])\n",
    "plt.xticks(correlation_nutrients.Category,nutrients_fraction, rotation='vertical',fontsize=15)\n",
    "plt.axhline(y=0,color='black') \n",
    "plt.ylabel('R',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense w.r.t. the WHO recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to analyse these results ! \n",
    "\n",
    "- Explain what the numbers mean \n",
    "- Explain that some correlations were expectables\n",
    "- Explain the surprising correlations as well. \n",
    "- Reasons why water is negatively correlated with income ? ( Everybody drinks it, but maybe the tap water quality is worse in poorer areas, so ppl need to buy bottled water )\n",
    "- Discuss the fact that some areas might have skewed data (very few people actually living there, which means that very few people actually do grocery shopping there ) \n",
    "- Maybe impact of other factors ? (minorities ... ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Regression : https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "Simple regression first and see the results, if logistic reg give good result nice OW try using more complex models. Check the overviews pages of SK-Learn. \n",
    "\n",
    "Check population density for MSOA/LSOA.\n",
    "\n",
    "Could look at the litterature on rel purchase and income, look at the finidings in litterature. \n",
    "\n",
    "Compare our results and compare differences or so -> Might lead to some new analysis to confirm results. \n",
    "\n",
    "Look for paper on google scholar but it's a bit hard and only looks at the Paper's title.\n",
    "Look at paper's citations in the paper and look at the ones in our fields. \n",
    "\n",
    "Also looked at the 'cited-by' field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will first try and fit a linear regressor to predict Mean and Median Income and see how well it performs. \n",
    "We keep 70% of our data for training. From the previous correlation studies, we decide to use as features only the columns for which the correlation is statistically significant. \n",
    "\n",
    "First we'll study the case for MSOA as it is the middle sized and we can witness the same phenomena for the three areas ! \n",
    "\n",
    "Hence we will use for training \n",
    "**f_beer, f_dairy, f_fats_oils, f_fish, f_fruit_veg, f_grains, f_meat_red, f_soft_drinks, f_spirits, f_sweets, f_tea_coffee, f_water and f_wine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared value is quite high ( 0.5-0.6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split our data for regression \n",
    "train, test = model_selection.train_test_split(groceries_income_ward, test_size=0.3)\n",
    "\n",
    "# Select the parameters columns\n",
    "\n",
    "columns = ['f_beer', 'f_dairy', 'f_fats_oils', 'f_fish', 'f_fruit_veg', 'f_grains',\n",
    "           'f_meat_red', 'f_soft_drinks', 'f_spirits', 'f_sweets', 'f_tea_coffee', 'f_water', 'f_wine']\n",
    "\n",
    "# Build X and Y for the training set \n",
    "X_tr = train[columns]\n",
    "y_tr = train['mean_income']\n",
    "\n",
    "# Fit the model \n",
    "reg_rd = linear_model.LinearRegression().fit(X_tr, y_tr)\n",
    "\n",
    "# Build X and Y for the test set\n",
    "X_te = test[columns]\n",
    "y_te = test['mean_income']\n",
    "\n",
    "# Predict using our model and evaluate the R-squared score\n",
    "y_pred = reg_rd.predict(X_te)\n",
    "r2 = reg_rd.score(X_te, y_te)\n",
    "print(f'R^2 score in the random split scenario : {r2}')\n",
    "\n",
    "residual = y_te - y_pred\n",
    "lm = sns.displot(residual,kind='kde')\n",
    "plt.xlabel('Residual values')\n",
    "plt.title('Distribution of the residuals for the test set', fontsize=20, y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability \n",
    "def CI_2_points_off(data, nbr_draws):\n",
    "    \"\"\" Bootstrap confidence interval that our regression is off by more than 2 points\"\"\"\n",
    "    percentage = np.zeros(nbr_draws)\n",
    "    data = np.array(data)\n",
    "\n",
    "    for n in range(nbr_draws):\n",
    "        indices = np.random.randint(0, len(data), len(data))\n",
    "        data_tmp = data[indices] \n",
    "        percentage[n] = ((data_tmp <= -5000) | (5000 <= data_tmp)).sum() / len(data)\n",
    "    \n",
    "    return np.nanpercentile(percentage, 2.5), np.median(percentage), np.nanpercentile(percentage, 97.5)\n",
    "\n",
    "lower, median, upper = CI_2_points_off(residual, 1000)\n",
    "print(f'The probability that our prediction is off by more than 5000 pounds is : {median:.5}')\n",
    "print(f'The confidence interval is : ({lower:.5}, {upper:.5})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(X_tr, y_tr, X_te, y_te)\n",
    "ridge_model(X_tr, y_tr, X_te, y_te)\n",
    "svm_model(X_tr, y_tr, X_te, y_te)\n",
    "tree_model(X_tr, y_tr, X_te, y_te)\n",
    "GBR_model(X_tr, y_tr, X_te, y_te)\n",
    "ADA_model(X_tr, y_tr, X_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the highest $R^2$ value for the Linear model so we will go on with this regressor for the rest of our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = linear_regression(X_tr, y_tr, X_te, y_te)\n",
    "\n",
    "pd.DataFrame({'Actual_Value': y_te, 'Predicted_Value': predicted_y})\n",
    "\n",
    "import seaborn as sns \n",
    "sns.regplot(x=predicted_y, y=y_te)\n",
    "plt.xlabel(\"Predicted Mean Income\")\n",
    "plt.ylabel(\"Actual Mean Income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "logitfit3 = smf.ols(formula = 'mean_income ~ f_beer + f_dairy + f_fats_oils + f_fish + f_fruit_veg + f_grains + f_meat_red + f_soft_drinks + f_spirits + f_sweets + f_tea_coffee + f_water + f_wine', data = groceries_income_ward).fit()\n",
    "print(logitfit3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "\n",
    "- Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations : \n",
    "\n",
    "- We are using data for 2012/13 for income Data, even though we can deduce insights thanks to our assumption, having data from 2015 would've been better ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA] *",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
